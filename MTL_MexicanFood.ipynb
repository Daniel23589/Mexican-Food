{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4228b7ac",
   "metadata": {},
   "source": [
    "# Primera parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815121f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "\n",
    "print(os.listdir(\"MAFood121\"))\n",
    "\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(42) # try and make the results more reproducible\n",
    "BASE_PATH = 'MAFood121/'\n",
    "\n",
    "epochs = 35\n",
    "batch_size = 64\n",
    "MICRO_DATA = True # very small subset (just 3 groups)\n",
    "SAMPLE_TRAINING = False # make train set smaller for faster iteration\n",
    "IMG_SIZE = (384, 384) # Try to change the model to U-net to avoid the resizing\n",
    "\n",
    "#Classes of dishes\n",
    "f = open(BASE_PATH + '/annotations/dishes.txt', \"r\")\n",
    "classes = f.read().strip().split('\\n')\n",
    "f.close()\n",
    "print(\"***** classes = dishes.txt: ***** \" + str(classes))\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "#Ingredients for each class\n",
    "f = open(BASE_PATH + '/annotations/foodgroups.txt', \"r\")\n",
    "ingredients = list(set(f.read().strip().split('\\n')))\n",
    "f.close()\n",
    "print(\"***** ingredients = foodgroups.txt: ***** \" + str(ingredients))\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "#Base Ingredients\n",
    "f = open(BASE_PATH + '/annotations/baseIngredients.txt', \"r\")\n",
    "base_ing = f.read().strip().split('\\n')\n",
    "f.close()\n",
    "print(\"***** base_ing = baseIngredients.txt: ***** \" + str(base_ing))\n",
    "print(\"#######################################################################################\")\n",
    "\n",
    "#Recovery of annotations ML\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "#train\n",
    "f = open(BASE_PATH + '/annotations/train.txt', \"r\")\n",
    "train_images = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open(BASE_PATH + '/annotations/train_lbls_ff.txt', \"r\")\n",
    "train_labels = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "#test\n",
    "f = open(BASE_PATH + '/annotations/test.txt', \"r\")\n",
    "test_images = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open(BASE_PATH + '/annotations/test_lbls_ff.txt', \"r\")\n",
    "test_labels = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "#val\n",
    "f = open(BASE_PATH + '/annotations/val.txt', \"r\")\n",
    "val_images = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open(BASE_PATH + '/annotations/val_lbls_ff.txt', \"r\")\n",
    "val_labels = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "#Recovery of annotations SL\n",
    "#train\n",
    "f = open(BASE_PATH + '/annotations/train.txt', \"r\")\n",
    "train_imagessl = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open(BASE_PATH + '/annotations/train_lbls_d.txt', \"r\")\n",
    "train_labelssl = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "#val\n",
    "f = open(BASE_PATH + '/annotations/val.txt', \"r\")\n",
    "val_imagessl = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open(BASE_PATH + '/annotations/val_lbls_d.txt', \"r\")\n",
    "val_labelssl = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "#test\n",
    "f = open(BASE_PATH + '/annotations/test.txt', \"r\")\n",
    "test_imagessl = f.read().split('\\n')\n",
    "f.close()\n",
    "f = open(BASE_PATH + '/annotations/test_lbls_d.txt', \"r\")\n",
    "test_labelssl = f.read().split('\\n')\n",
    "f.close()\n",
    "\n",
    "# Single-Label\n",
    "train_images_sl = [\"MAFood121/images/\" + s for s in train_imagessl]\n",
    "train_df_sl = pd.DataFrame({'path': train_images_sl, 'sl_class_id': train_labelssl})\n",
    "\n",
    "val_images_sl = [\"MAFood121/images/\" + s for s in val_imagessl]\n",
    "val_df_sl = pd.DataFrame({'path': val_images_sl, 'sl_class_id': val_labelssl})\n",
    "print(val_df_sl)\n",
    "\n",
    "test_images_sl = [\"MAFood121/images/\" + s for s in test_imagessl]\n",
    "test_df_sl = pd.DataFrame({'path': test_images_sl, 'sl_class_id': test_labelssl})\n",
    "\n",
    "# Multi-label\n",
    "train_images_ml = [\"MAFood121/images/\" + s for s in train_images]\n",
    "train_df_ml = pd.DataFrame({'path': train_images_ml, 'ml_class_id': train_labels})\n",
    "\n",
    "val_images_ml = [\"MAFood121/images/\" + s for s in val_images]\n",
    "val_df_ml = pd.DataFrame({'path': val_images_ml, 'ml_class_id': val_labels})\n",
    "\n",
    "test_images_ml = [\"MAFood121/images/\" + s for s in test_images]\n",
    "test_df_ml = pd.DataFrame({'path': test_images_ml, 'ml_class_id': test_labels})\n",
    "\n",
    "# Dataframe for train images\n",
    "import glob\n",
    "\n",
    "train_ingredients = []\n",
    "train_classid = []\n",
    "\n",
    "# busca ambos archivos en el directorio de anotaciones\n",
    "for file_path in glob.glob(BASE_PATH + '/annotations/train_lbls_*.txt'):\n",
    "    with open(file_path) as f1:\n",
    "        for line in f1:\n",
    "            idx_ingredients = []\n",
    "            classid = int(line)\n",
    "            train_classid.append(classid)\n",
    "            for ing in ingredients[classid].strip().split(\",\"):\n",
    "                idx_ingredients.append(str(base_ing.index(ing)))\n",
    "            train_ingredients.append(idx_ingredients)\n",
    "\n",
    "df_train = pd.DataFrame(mlb.fit_transform(train_ingredients), columns=mlb.classes_) #binary encode ingredients\n",
    "df_train[\"path\"] = train_df_ml['path'] #train_img_df['path']\n",
    "df_train[\"ml_class_id\"] = train_classid \n",
    "food_dict_train = df_train\n",
    "\n",
    "new_data = []\n",
    "for index, row in train_df_ml.iterrows():\n",
    "    #food = row[\"class_name\"]\n",
    "    path = row[\"path\"]\n",
    "    class_id = row[\"ml_class_id\"]\n",
    "    \n",
    "    binary_encod = food_dict_train.loc[food_dict_train[\"path\"] == path]\n",
    "    new_data.append(np.array(binary_encod)[0])\n",
    "\n",
    "col_names = list(binary_encod.columns.values)\n",
    "train_df = pd.DataFrame(new_data, columns = col_names)\n",
    "\n",
    "#Dataframe for val images\n",
    "val_ingredients = []\n",
    "val_classid = []\n",
    "\n",
    "# busca ambos archivos en el directorio de anotaciones\n",
    "for file_path in glob.glob(BASE_PATH + '/annotations/val_lbls_*.txt'):\n",
    "    with open(file_path) as f1:\n",
    "        for line in f1:\n",
    "            idx_ingredients = []\n",
    "            classid = int(line)\n",
    "            val_classid.append(classid)\n",
    "            for ing in ingredients[classid].strip().split(\",\"):\n",
    "                idx_ingredients.append(str(base_ing.index(ing)))\n",
    "            val_ingredients.append(idx_ingredients)\n",
    "\n",
    "df_val = pd.DataFrame(mlb.fit_transform(val_ingredients), columns=mlb.classes_) #binary encode ingredients\n",
    "df_val[\"path\"] = val_df_ml['path']\n",
    "df_val[\"ml_class_id\"] = val_classid \n",
    "food_dict_val = df_val\n",
    "\n",
    "\n",
    "new_data = []\n",
    "for index, row in val_df_ml.iterrows():\n",
    "    #food = row[\"class_name\"]\n",
    "    path = row[\"path\"]\n",
    "    class_id = row[\"ml_class_id\"]\n",
    "    \n",
    "    binary_encod = food_dict_val.loc[food_dict_val[\"path\"] == path]\n",
    "    new_data.append(np.array(binary_encod)[0])\n",
    "\n",
    "col_names = list(binary_encod.columns.values)\n",
    "val_df = pd.DataFrame(new_data, columns = col_names)\n",
    "\n",
    "#Dataframe for test images\n",
    "test_ingredients = []\n",
    "test_classid = []\n",
    "\n",
    "# busca ambos archivos en el directorio de anotaciones\n",
    "for file_path in glob.glob(BASE_PATH + '/annotations/test_lbls_*.txt'):\n",
    "    with open(file_path) as f1:\n",
    "        for line in f1:\n",
    "            idx_ingredients = []\n",
    "            classid = int(line)\n",
    "            test_classid.append(classid)\n",
    "            for ing in ingredients[classid].strip().split(\",\"):\n",
    "                idx_ingredients.append(str(base_ing.index(ing)))\n",
    "            test_ingredients.append(idx_ingredients)\n",
    "\n",
    "df_test = pd.DataFrame(mlb.fit_transform(test_ingredients), columns=mlb.classes_) #binary encode ingredients\n",
    "df_test[\"path\"] = test_df_ml['path']\n",
    "df_test[\"ml_class_id\"] = test_classid \n",
    "food_dict_test = df_test\n",
    "\n",
    "\n",
    "new_data = []\n",
    "for index, row in test_df_ml.iterrows():\n",
    "    #food = row[\"class_name\"]\n",
    "    path = row[\"path\"]\n",
    "    class_id = row[\"ml_class_id\"]\n",
    "    \n",
    "    binary_encod = food_dict_test.loc[food_dict_test[\"path\"] == path]\n",
    "    new_data.append(np.array(binary_encod)[0])\n",
    "\n",
    "col_names = list(binary_encod.columns.values)\n",
    "test_df = pd.DataFrame(new_data, columns = col_names)\n",
    "\n",
    "train_df = train_df.merge(train_df_sl, left_on='path', right_on='path')\n",
    "val_df = val_df.merge(val_df_sl, left_on='path', right_on='path') \n",
    "test_df = test_df.merge(test_df_sl, left_on='path', right_on='path')\n",
    "\n",
    "train_df.to_hdf('train_df.h5','df',mode='w',format='table',data_columns=True)\n",
    "val_df.to_hdf('val_df.h5','df',mode='w',format='table',data_columns=True)\n",
    "test_df.to_hdf('test_df.h5','df',mode='w',format='table',data_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac42c5",
   "metadata": {},
   "source": [
    "# segunda parte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac46c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics loss and accuracy\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "\n",
    "# Load data from .h5 files\n",
    "train_df = pd.read_hdf('train_df.h5', 'df')\n",
    "val_df = pd.read_hdf('val_df.h5', 'df')\n",
    "test_df = pd.read_hdf('test_df.h5', 'df')\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 16\n",
    "SMALL_DATA = False\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "if SMALL_DATA:\n",
    "    train_df = train_df[:128]\n",
    "    val_df = test_df[:128]\n",
    "    test_df = test_df[:128]\n",
    "\n",
    "col_names = list(train_df.columns.values)\n",
    "ing_names = col_names[:-3]\n",
    "targets = ing_names\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.df.iloc[idx]['path']\n",
    "        try:\n",
    "            image = cv2.imread(image_path, 1)\n",
    "            if image is None:\n",
    "                raise ValueError(f\"Failed to read image at {image_path}\")\n",
    "            if image.shape[0] == 0 or image.shape[1] == 0:\n",
    "                raise ValueError(f\"Invalid image size for {image_path}\")\n",
    "\n",
    "            x = cv2.resize(image, IMG_SIZE)\n",
    "            x = torch.from_numpy(x.transpose(2, 0, 1)).float()\n",
    "\n",
    "            sl_class_id = int(self.df.iloc[idx]['sl_class_id'])\n",
    "            sl_onehot = np.array(sl_class_id)\n",
    "            sl_onehot = (np.arange(len(classes)) == sl_onehot).astype(np.float32)\n",
    "            sl_y = torch.from_numpy(sl_onehot)\n",
    "\n",
    "            ml_y = []\n",
    "            for i in range(len(base_ing)):\n",
    "                ml_y.append(self.df.iloc[idx][str(i)])\n",
    "            ml_y = np.array(ml_y, dtype=np.float32)\n",
    "\n",
    "            return (x, sl_y, ml_y)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading image at {image_path}: {str(e)}\")\n",
    "            # Devuelve un valor predeterminado o imagen vacía\n",
    "            x = torch.zeros((3, IMG_SIZE[0], IMG_SIZE[1])).float()\n",
    "            sl_y = torch.zeros(len(classes)).float()\n",
    "            ml_y = np.zeros(len(base_ing), dtype=np.float32)\n",
    "            return (x, sl_y, ml_y)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create DataLoader objects for training, validation, and testing sets\n",
    "train_dataset = CustomDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = CustomDataset(val_df)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "test_dataset = CustomDataset(test_df)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# ResNet50 Model\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "# Disable grad for all conv layers\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add two heads\n",
    "resnet.last_linear = resnet.fc\n",
    "n_features = resnet.fc.out_features\n",
    "head_sl = nn.Sequential(\n",
    "    nn.Linear(n_features, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(512, len(classes))\n",
    ")\n",
    "head_ml = nn.Sequential(\n",
    "    nn.Linear(n_features, 512),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(512, len(base_ing)),\n",
    "    nn.Sigmoid()\n",
    ")  \n",
    "\n",
    "# Connect two heads\n",
    "class FoodModel(nn.Module):\n",
    "    def __init__(self, base_model, head_sl, head_ml):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.head_sl = head_sl\n",
    "        self.head_ml = head_ml\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        sl = self.head_sl(x)\n",
    "        ml = self.head_ml(x)\n",
    "        return sl, ml\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = FoodModel(resnet, head_sl, head_ml)\n",
    "model.to(device)\n",
    "\n",
    "# Define Loss\n",
    "sl_loss_fn = nn.CrossEntropyLoss()\n",
    "ml_loss_fn = nn.BCELoss()\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Define function to calculate accuracy for both SL and ML tasks\n",
    "def calculate_accuracy(preds, targets, task='sl'):\n",
    "    if task == 'sl':\n",
    "        predicted_labels = torch.argmax(preds, dim=1)\n",
    "        correct_predictions = (predicted_labels == targets).sum().item()\n",
    "    elif task == 'ml':\n",
    "        predicted_labels = (preds > 0.5).float()  # Threshold predictions for multi-label task\n",
    "        correct_predictions = (predicted_labels == targets).all(dim=1).sum().item()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid task. It should be 'sl' or 'ml'.\")\n",
    "\n",
    "    total_samples = targets.size(0)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    return accuracy\n",
    "\n",
    "# Modify the train_step function to include accuracy calculation\n",
    "def train_step(model, optimizer, sl_loss_fn, ml_loss_fn, data, device):\n",
    "    # Retrieve data\n",
    "    x, sl_y, ml_y = data\n",
    "\n",
    "    # Convert to device\n",
    "    x = x.to(device)\n",
    "    sl_y = sl_y.to(device)\n",
    "    ml_y = ml_y.to(device)\n",
    "\n",
    "    # Zero out gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    sl_preds, ml_preds = model(x)\n",
    "\n",
    "    # Calculate losses\n",
    "    sl_loss = sl_loss_fn(sl_preds, torch.argmax(sl_y, dim=1))\n",
    "    ml_loss = ml_loss_fn(ml_preds, ml_y)\n",
    "    loss = sl_loss + ml_loss\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Step optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    # Calculate accuracies\n",
    "    sl_accuracy = calculate_accuracy(sl_preds, torch.argmax(sl_y, dim=1), task='sl')\n",
    "    ml_accuracy = calculate_accuracy(ml_preds, ml_y, task='ml')\n",
    "\n",
    "    # Return losses and accuracies\n",
    "    return sl_loss.item(), ml_loss.item(), sl_accuracy, ml_accuracy\n",
    "\n",
    "# Lists to store losses and accuracies for plotting\n",
    "train_lossessl = []\n",
    "train_lossesml = []\n",
    "train_accuraciessl = []\n",
    "train_accuraciesml = []\n",
    "\n",
    "epochs = 10\n",
    "for i in tqdm(range(epochs), desc='Epochs'):\n",
    "    print(\"Epoch \", i)\n",
    "    total_sl_loss = 0.0\n",
    "    total_ml_loss = 0.0\n",
    "    total_sl_accuracy = 0.0\n",
    "    total_ml_accuracy = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with tqdm(train_loader, desc='Training', total=len(train_loader), miniters=1) as pbar:\n",
    "        for data in pbar: \n",
    "            SL_loss, ML_loss, SL_accuracy, ML_accuracy = train_step(model, optimizer, sl_loss_fn, ml_loss_fn, data, device)\n",
    "\n",
    "            total_sl_loss += SL_loss\n",
    "            total_ml_loss += ML_loss\n",
    "            total_sl_accuracy += SL_accuracy\n",
    "            total_ml_accuracy += ML_accuracy\n",
    "            total_samples += data[0].size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'SL Loss': total_sl_loss / total_samples,\n",
    "                'ML Loss': total_ml_loss / total_samples,\n",
    "                'SL Accuracy': total_sl_accuracy / total_samples,\n",
    "                'ML Accuracy': total_ml_accuracy / total_samples,\n",
    "            })\n",
    "\n",
    "    # Calculate average losses and accuracies\n",
    "    avg_sl_loss = total_sl_loss / len(train_loader)\n",
    "    avg_ml_loss = total_ml_loss / len(train_loader)\n",
    "    avg_sl_accuracy = total_sl_accuracy / len(train_loader)\n",
    "    avg_ml_accuracy = total_ml_accuracy / len(train_loader)\n",
    "\n",
    "    # Append losses and accuracies to the lists\n",
    "    train_lossessl.append(avg_sl_loss)\n",
    "    train_lossesml.append(avg_ml_loss)\n",
    "    train_accuraciessl.append(avg_sl_accuracy)\n",
    "    train_accuraciesml.append(avg_ml_accuracy)\n",
    "\n",
    "# Plot loss and accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_lossessl, label='Train Loss SL')\n",
    "plt.plot(train_lossesml, label='Train Loss ML')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuraciessl, label='Train Accuracy SL')\n",
    "plt.plot(train_accuraciesml, label='Train Accuracy ML')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Load a test image\n",
    "img_path = '2352494.jpg'\n",
    "img = Image.open(img_path).convert('RGB')\n",
    "plt.imshow(img)\n",
    "\n",
    "# Resize image and convert to tensor\n",
    "transform = transforms.Compose([transforms.Resize(IMG_SIZE), transforms.ToTensor()])\n",
    "img = transform(img)\n",
    "img = img.unsqueeze(0)\n",
    "\n",
    "# Get model predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sl_preds, ml_preds = model(img.to(device))\n",
    "\n",
    "sl_preds = torch.nn.functional.softmax(sl_preds)\n",
    "sl_preds = sl_preds.cpu().numpy()\n",
    "ml_preds = ml_preds.cpu().numpy()\n",
    "\n",
    "# Plot prediction results\n",
    "sl_preds = sl_preds.squeeze()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(classes, sl_preds)\n",
    "plt.title('Softmax Prediction')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Food Category')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()\n",
    "\n",
    "ml_preds = ml_preds.squeeze()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(base_ing, ml_preds)\n",
    "plt.title('Sigmoid Prediction')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Ingredient')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
